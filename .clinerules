# 本プロジェクトの目的
## プロジェクトゴール

- ユーザーはアークナイツのwikiにある情報を収集し、RAGを活用して一般的な情報を踏まえつつも最新の情報を含めた回答ができるようになるシステムの作成をしようとしています

- 私はそのためにただ動くだけではなく、後々メンテナスしやすいことや、RAGを使ったことによる評価がしやすいシステムを目指す

## 制約事項

- openai等のAPIを使う場合、コストには気をつけます。必要に応じてユーザに確認をしながら進めます。

# プロジェクト内で使う技術的なTips

## Pythonのバージョンやパッケージの管理

- uv(https://docs.astral.sh/uv/) を用いる
- uvの使い方
  - 初期化 `uv init`
  - Pythonのインストール `uv python install 3.xx.xx`
    - Pythonのバージョンは以下ページでbugfixフェーズの最新のものを使う
      - https://devguide.python.org/versions/
  - Pythonバージョンの固定 `uv python pin 3.xx.xx` => .python-version ファイルが作成される
  - 仮想環境の作成 `uv venv`
  - パッケージの追加 `uv add package_name`
  - 作成したプログラムの実行 `uv run python xxx.py`
- パッケージなどの情報はrequirements.txtでなく、pyproject.tomlに書かれます。

## 関数の書き方

- 私は以下を心がけます
  - ユーザはテストにおいてはいわゆる古典学派なのでできるだけ純粋関数になるようなことを考えて実装してください
  - 外部への依存性はまとめてください
  - いわゆるクリーンアーキテクチャやオニオンアーキテクチャが理想形ではありますが、実装がそこまで大きくないので、それを意識した程度のアーキテクチャになるようにします

- 型は必ず書いてください

- docstringは必ず書いてください。なお、google-styleでお願いします。

## テスト

### テストの書き方

- 純粋関数に対する単体テストは必ず書いてください。リポジトリ系については単体テストの必要はあまりないと考えています。
- メインロジックに対するテストは必ず書きます。リポジトリ系の関数はMockしてMockされた状態で純粋関数的に振る舞うようにしてください。

### カバレッジ

- 上記の通りなので絶対的な目標は定めませんが、概ね80%前後を目標としてください。


## 記憶量とデータの読み方

私の記憶領域は有限であり、巨大なファイルを読み込まないようにする必要があります。

`*.json` や `*.jsonl`, `*.html` を read-file する前には、`ls -al <file>` でファイルサイズを確認してください。

また、大きなファイルを読み込む必要がどうしてもある場合は、サマライズする方法を考えたうえでユーザに提案してください。
例) htmlを読み込みたいが大きいので、BeautifulSoupでbodyタグないの日本語のみを抽出したうえで読み込む

# 仕様
## エントリーポイント

CLIから呼び出すときのエントリーポイントとして機能し、引数や環境変数等を処理し、クローリング、前処理などの各関数を呼び出す

呼び出し例

```
uv run python -m arknights_crawler.cli crawl --gcp_project_id hogehoge --gcs_bucket_name fugafuga
```

## クローリング

クロール対象: https://arknights.wikiru.jp/index.php?plugin=sitemap  で表示されるsitemap

出力
- htmlファイル- インデックスを記録したjsonl

取得の仕様
- 前回取得より更新されているものを取得する- 更新されたものの中に同じページURLがあった場合は別のidで管理する。
  - この場合、URLと取得時刻をもって最新のデータを判別できる状態とする出力の仕様
- html
  - 配置場所はGCS上の html/{id}.html
  - htmlのファイル名はそのままの名前ではなく、id等で扱う(ファイル名で受け入れられる長さがURLより短いため
- jsonl(メタ情報)
  - 配置場所はGCS上の meta/index_{YYYYMMDDhhmmss}_{chunk_id}.jsonl
  - id と 実ファイル名の対応付けを保存する
  - 記録する内容は id, filename, fetched_at
  - ある程度の単位でファイルは分割して問題ない

引数
- sitemap_url: サイトマップのURL(デフォルト値が指定されており、通常固定値)
- output_dir: 取得したhtmlを保存するディレクトリ名のプレフィックス
- meta_output_dir: メタデータ(JSONL)を保存するプレフィックス
- meta_chunk_size: メタデータのレコード(JSONL)をファイル出力する単位
- gcs_project: Google Cloud Storageのあるプロジェクト名
- gcp_bucket_name: Google Cloud Storageのバケット名

出力先
- html/jsonlともにGCS上にアップロードする
- できるだけ細かい単位でアップロードする
  - テスト時に簡単に確認できるように分ける単位を選択できることが望ましい

## データの前処理

背景
- 取得したhtmlはメニュー部分などどのページにも入っている不要な情報が多くあるので除去したい

対象データ
- クローリングで獲得したデータ。GCSの特定ディレクトリ(html/)以下にある。

抽出対象
- `div#body` のタグ配下のhtml。htmlタグも有効な情報になり得るのでタグの除去はしない

出力先
- 指定されたGCS上の output/extracted/{id}.html に抽出された部分のみを保存する

メタデータ
- 以下のデータを出力する
  - id: 自動採番
  - original_html_id: crawler側のidへのFK相当
  - processed_at: 処理した時刻
- output/meta/extracted_{YYYYMMDDhhmss}.jsonl として保存する

引数
- --input-dir: 元となるhtmlがあるディレクトリ(通常はcrawler.pyの出力先に等しい)
- --meta-chunk-size: メタデータのレコード(JSONL)をファイル出力する単位
- --gcs-project: Google Cloud Storageのあるプロジェクト名
- --gcs-bucket-name: in/outに使うGCSのバケット名

# 設計
## ソフトウェアアーキテクチャ

- クリーンアーキテクチャのような重たいアーキテクチャは導入しないが、ある程度意識した構造にする
- 関数はできるだけ純粋関数的に実装し、入出力はしっかり分ける
  - ファイルやクラウドサービスへの外部入出力はrepositoryクラスや関数として切り出す
  - 実際の処理をする関数への値渡しはパラメータ経由のみとし、グローバル変数等は使わない
  - 実行時の引数や環境変数はある程度一箇所でパースし、入力されたデータと実装を切り離す
- 機能ごとにディレクトリを切り、その中でファイルは凝集性を意識した単位でわける
